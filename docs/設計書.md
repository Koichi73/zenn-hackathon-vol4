# 基本設計

## MVP機能要件
- 作成者
    - 動画をアップロードする
    - 動画から手順ごとのテキストを生成する
    - 動画から手順ごとの画像を切り抜く
    - 画像内の個人情報をマスクする
    - 画像内のボタンを赤枠で強調する
    - AIの出力を手動で変更する
    - 作成者権限でログインする
    - 編集モードとプレビューモードがある
    - 利用者が指摘した箇所を確認する
    - 利用者に手順書を共有する(URLでの共有)
    - 手順書をPDFでダウンロードする(開発せずに印刷機能を使う)
- 利用者
    - 手順書を閲覧できる(要検討: ログインなしもしくは閲覧権限でのログイン)
    - 手順書の古い箇所を1クリックで指摘する
        - TeamsやSlackのスタンプorYoutubeの低評価ボタンみたいなイメージ
    - 指摘の際にコメントを残す


## 非MVP機能要件
- 作成者
    - 追加の動画で部分的な差分をとる
    - 入力動画が綺麗ではない時の対応
        - 動画内で間違った手順が含まれてる、手戻りがある、など
    - 手順書と動画のシンクロ
        - テキストや切り出し画像をクリックすると動画がその位置に飛ぶ
        - 動画のシークバーを動かすと手順書の該当箇所に飛ぶ
- 利用者
    - 他人の指摘の履歴を確認する
        - 要検討: MVPに入れてもいいかも


## 技術スタック
- バックエンド
    - フレームワーク: FastAPI
    - AI: Vertex AI (Gemini 3)
        - 動画などのマルチモーダルに強い
    - Agentフレームワーク: ADK
    - 画像処理: ffmpeg
- フロントエンド
    - フレームワーク: Next.js
    - UI: TailwindCSS
    - Canvas: react-konva
    - 動画アップロード: Firebase SDK
        - サーバを経由せず直接アップロードする
- データベース
    - Firestore
        - テキストやユーザー情報など
        - 動画はCloud StrageのURLとして保存する
    - Cloud Storage
        - 動画と画像(これらはFirestoreだと大きすぎる)
        - GeminiはCloud Strageの動画を解析できる
        - 動画 → フロント → Cloud Storage → Gemini の流れ
- デプロイ 
    - フロントエンド: Cloud Run もしくは Firebase Hosting
        - Cloud Run: 
            - 統一感ある
            - Dockerfile書くのは問題ない
            - デブロイが遅い
        - Firebase Hosting: 
            - デプロイが速い
            - 裏でCloud Runを使ってる
            - デメリットはあまりない
    - バックエンド: Cloud Run


## アーキテクチャ図(MVP)

```mermaid
graph LR
    User((User))
    
    subgraph Frontend ["Frontend (Cloud Run / Firebase Hosting)"]
        NextJS["Next.js App<br/>(Firebase SDK)"]
    end

    subgraph Google_Cloud ["Google Cloud Platform"]
        subgraph Backend ["Backend (Cloud Run)"]
            FastAPI[FastAPI Server]
            ADK[ADK Agent Logic]
        end
        
        Storage["Cloud Storage (GCS)"]
        DB[(Firestore)]
        Gemini["Vertex AI (Gemini 3)"]
    end

    %% 詳細なフロー
    User -- 1. 動画選択 & UP --> NextJS
    NextJS -- 2. 直接アップロード<br/>(Server経由せず) --> Storage
    NextJS -- 3. 解析リクエスト<br/>(動画パスを送信) --> FastAPI
    FastAPI -- 4. 動画URI参照 --> Gemini
    Gemini -- 5. 手順テキスト --> FastAPI
    FastAPI -- 6. 保存 --> DB
    FastAPI -- 7. 完了通知 --> NextJS
```


## 動画アップロードフロー
```mermaid
sequenceDiagram
    autonumber
    actor User as 👤 ユーザー
    participant UI as 💻 フロントエンド<br/>(Next.js)
    participant Cloud as ☁️ Firebase<br/>(Storage / Firestore)
    participant API as 🤖 バックエンド<br/>(Cloud Run + FastAPI)

    Note over User, Cloud: 【Phase 1】 アップロード & 準備
    User->>UI: 動画ファイルを選択
    UI->>Cloud: 1. 動画を直接アップロード (Storage SDK)
    activate Cloud
    Cloud-->>UI: 保存完了 (URL取得)
    deactivate Cloud
    UI->>Cloud: 2. ジョブ作成 { status: "queued", url: ... } (Firestore)
    
    Note over UI, API: 【Phase 2】 解析依頼 (非同期キック)
    UI->>API: 3. 解析開始リクエスト (POST /analyze)
    activate API
    API->>API: BackgroundTasksに登録
    API-->>UI: 4. "202 Accepted" (即レス・切断)
    deactivate API

    par ユーザー体験と裏側処理の並走
        Note left of UI: 【Phase 3-A】 監視 (UI)
        loop Firestore Realtime Listener
            Cloud-->>UI: 5. ステータス変更通知 (onSnapshot)
            Note left of UI: 通知が来るたびに<br/>画面の進捗バーや<br/>メッセージを更新
        end

        Note right of API: 【Phase 3-B】 解析実行 (Backend)
        rect rgb(240, 248, 255)
            API->>API: 6. 動画全体解析 (Gemini)
            API->>Cloud: ステータス更新: "analyzing_video"
            
            API->>API: 7. 重要シーンのタイムスタンプ特定
            API->>Cloud: ステータス更新: "extracting_frames"
            
            API->>API: 8. フレーム切り出し & 詳細解析
            API->>Cloud: ステータス更新: "analyzing_frames"
            
            API->>Cloud: 9. 完了データ保存 & ステータス: "completed"
        end
    end

    Note over User, UI: 【Phase 4】 ゴール
    UI->>UI: "completed" を検知
    UI->>User: 10. 解析結果画面を表示！
```


## アーキテクチャ図(ストリーミング)

```mermaid
sequenceDiagram
    participant U as User
    participant FE as Next.js (UI)
    participant BE as FastAPI
    participant AI as Gemini
    participant DB as Firestore

    U->>FE: 動画アップロード
    FE->>BE: 解析リクエスト
    
    rect rgb(230, 240, 255)
        Note over BE, AI: Phase 1: 構造解析 (数秒)
        BE->>AI: 動画全体からタイトル/時間抽出
        AI-->>BE: ステップリスト (JSON)
    end

    BE->>DB: 初期データ保存 (タイトルのみ)
    BE-->>FE: 200 OK (処理開始)
    
    Note right of FE: ユーザーには「5つの手順を検出しました...」<br/>とスケルトンを表示

    loop 各ステップごとに実行 (Background)
        BE->>BE: 画像切り抜き (ffmpeg)
        BE->>AI: 画像解析 (Phase 3)
        AI-->>BE: マスク位置・説明文
        BE->>DB: update() 特定のステップを更新
        DB-->>FE: onSnapshot (リアルタイム反映)
        Note right of FE: 1つずつ画像と説明が<br/>UIにポコポコ表示される
    end
```


## ページ構成
```
/
├── /login          # ログイン画面
├── /dashboard      # ダッシュボード (一覧表示、フィードバック通知、アップロード)
├── /editor/[id]    # 編集画面
└── /share/[id]     # 手順書閲覧画面
```
